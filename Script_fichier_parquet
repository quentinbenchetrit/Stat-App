#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import logging
import os
import sys
from pathlib import Path
import duckdb

LOG = logging.getLogger("merge_patents_ep7")

DEFAULT_BASE = "/home/onyxia/work/"
DEFAULT_CSV = str(Path(DEFAULT_BASE) / "list_cpc_patent_EP.csv")
DEFAULT_TXT = str(Path(DEFAULT_BASE) / "202401_EPO_App_reg.txt")
DEFAULT_OUT = str(Path(DEFAULT_BASE) / "patents_merged.parquet")
DEFAULT_DB  = str(Path(DEFAULT_BASE) / "work.duckdb")

def parse_args():
    p = argparse.ArgumentParser(
        description=(
            "Merge EP CSV (CPC4) and TXT (applicants) using the EP document number: "
            "extracted from publication_number in CSV and taken from pub_nbr in TXT."
        )
    )
    p.add_argument("--csv", default=DEFAULT_CSV, help="Path to CSV (list_cpc_patent_EP.csv)")
    p.add_argument("--txt", default=DEFAULT_TXT, help="Path to TXT (202401_EPO_App_reg.txt, '|' delimited)")
    p.add_argument("--out", default=DEFAULT_OUT, help="Output Parquet path")
    p.add_argument("--duckdb", default=DEFAULT_DB, help="DuckDB database file (for spill/caching)")
    p.add_argument("--threads", type=int, default=os.cpu_count() or 4, help="DuckDB threads")
    return p.parse_args()

def fail(msg: str):
    LOG.error(msg)
    sys.exit(1)

def main():
    args = parse_args()
    logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")

    csv_path = Path(args.csv)
    txt_path = Path(args.txt)
    out_path = Path(args.out)

    if not csv_path.is_file():
        fail(f"CSV not found: {csv_path}")
    if not txt_path.is_file():
        fail(f"TXT not found: {txt_path}")
    out_path.parent.mkdir(parents=True, exist_ok=True)

    con = duckdb.connect(database=str(args.duckdb) if args.duckdb else ":memory:")
    con.execute(f"PRAGMA threads={args.threads}")
    con.execute("PRAGMA preserve_insertion_order=false")

    # 1) Charger les fichiers bruts
    con.execute(f"""
        CREATE OR REPLACE VIEW csv_raw AS
        SELECT * FROM read_csv_auto(
            '{csv_path.as_posix()}',
            SAMPLE_SIZE=-1,
            HEADER=TRUE
        );
    """)

    con.execute(f"""
        CREATE OR REPLACE VIEW txt_raw AS
        SELECT * FROM read_csv_auto(
            '{txt_path.as_posix()}',
            DELIM='|',
            QUOTE='\"',
            SAMPLE_SIZE=-1,
            HEADER=TRUE
        );
    """)

    # 2) Vérifier les colonnes attendues
    csv_cols = {c[0] for c in con.execute("SELECT * FROM csv_raw LIMIT 0").description}
    txt_cols = {c[0] for c in con.execute("SELECT * FROM txt_raw LIMIT 0").description}

    # Côté CSV : on utilise publication_number, publication_date, CPC4
    need_csv = {"publication_number", "publication_date", "CPC4"}
    # Côté TXT : la clé est basée sur pub_nbr (et on garde les autres métadonnées)
    need_txt = {
        "pub_nbr", "appln_id", "person_id", "app_name", "address", "city",
        "postal_code", "reg_code", "ctry_code", "reg_share", "app_share"
    }

    if not need_csv.issubset(csv_cols):
        fail(f"CSV missing columns. Found={sorted(csv_cols)} Needed>={sorted(need_csv)}")
    if not need_txt.issubset(txt_cols):
        fail(f"TXT missing columns. Found={sorted(txt_cols)} Needed>={sorted(need_txt)}")

    # 3) Comptages bruts
    csv_rows = con.execute("SELECT COUNT(*) FROM csv_raw").fetchone()[0]
    txt_rows = con.execute("SELECT COUNT(*) FROM txt_raw").fetchone()[0]
    LOG.info(f"CSV rows: {csv_rows:,} | TXT rows: {txt_rows:,}")

    # 4) Côté CSV : extraire un numéro de document entier (doc_number) depuis publication_number
    #    Exemple : EP-0021337-B1 -> 21337 ; EP-1070969-A1 -> 1070969
    con.execute("""
        CREATE OR REPLACE VIEW csv_keyed AS
        SELECT
            try_cast(
                regexp_extract(
                    publication_number,
                    '^EP-0*([0-9]{7})-[A-Z][0-9]$',
                    1
                ) AS BIGINT
            ) AS doc_number,
            publication_date AS publication_date,
            upper(trim(CPC4)) AS cpc4
        FROM csv_raw;
    """)

    # 5) Nettoyage & agrégation des CPC (uniques) par doc_number
    con.execute("""
        CREATE OR REPLACE VIEW csv_cpc AS
        WITH filtered AS (
            SELECT doc_number, publication_date, cpc4
            FROM csv_keyed
            WHERE doc_number IS NOT NULL
              AND cpc4 IS NOT NULL AND cpc4 <> ''
        ),
        pub_agg AS (
            -- Date de publication : on garde la plus ancienne
            SELECT doc_number, min(publication_date) AS publication_date
            FROM filtered
            GROUP BY doc_number
        ),
        cpc_distinct AS (
            SELECT DISTINCT doc_number, cpc4
            FROM filtered
        )
        SELECT
            p.doc_number,
            p.publication_date,
            list(cd.cpc4) AS cpc4_list
        FROM pub_agg p
        JOIN cpc_distinct cd USING (doc_number)
        GROUP BY p.doc_number, p.publication_date;
    """)

    csv_valid = con.execute("SELECT COUNT(*) FROM csv_cpc").fetchone()[0]
    LOG.info(f"CSV patents with doc_number & CPC: {csv_valid:,}")

    # 6) Côté TXT : construire doc_number depuis pub_nbr (7 chiffres)
    #    Exemple : pub_nbr = 1070969 -> doc_number = 1070969
    con.execute("""
        CREATE OR REPLACE VIEW txt_keyed AS
        SELECT
            try_cast(trim(pub_nbr) AS BIGINT) AS doc_number,
            person_id, app_name, address, city, postal_code, reg_code,
            ctry_code, reg_share, app_share
        FROM txt_raw
        WHERE pub_nbr IS NOT NULL
          AND trim(pub_nbr) <> '';
    """)

    # 7) Agréger les applicants par doc_number
    con.execute("""
        CREATE OR REPLACE VIEW applicants AS
        SELECT
            doc_number,
            list(person_id)   AS person_id_list,
            list(app_name)    AS app_name_list,
            list(address)     AS address_list,
            list(city)        AS city_list,
            list(postal_code) AS postal_code_list,
            list(reg_code)    AS reg_code_list,
            list(ctry_code)   AS ctry_code_list,
            list(reg_share)   AS reg_share_list,
            list(app_share)   AS app_share_list,
            count(*)          AS num_applicants
        FROM txt_keyed
        WHERE doc_number IS NOT NULL
        GROUP BY doc_number;
    """)

    txt_valid = con.execute("SELECT COUNT(*) FROM applicants").fetchone()[0]
    LOG.info(f"TXT applicants with doc_number (from pub_nbr): {txt_valid:,}")

    # 8) Jointure sur doc_number
    con.execute("""
        CREATE OR REPLACE VIEW final AS
        SELECT
            c.doc_number,
            lpad(cast(c.doc_number AS VARCHAR), 7, '0') AS doc7,
            'EP-' || lpad(cast(c.doc_number AS VARCHAR), 7, '0') AS ep_doc,
            c.publication_date,
            c.cpc4_list,
            a.person_id_list,
            a.app_name_list,
            a.address_list,
            a.city_list,
            a.postal_code_list,
            a.reg_code_list,
            a.ctry_code_list,
            a.reg_share_list,
            a.app_share_list,
            a.num_applicants
        FROM csv_cpc c
        INNER JOIN applicants a USING (doc_number)
        WHERE c.cpc4_list IS NOT NULL;
    """)

    matched = con.execute("""
        SELECT COUNT(*) FROM (
            SELECT doc_number FROM csv_cpc
            INTERSECT
            SELECT doc_number FROM applicants
        );
    """).fetchone()[0]
    final_count = con.execute("SELECT COUNT(*) FROM final").fetchone()[0]
    LOG.info(f"Matched on doc_number (intersection): {matched:,}")
    LOG.info(f"Final row count (CPC non-empty): {final_count:,}")

    # 9) Écriture Parquet
    LOG.info(f"Writing Parquet → {out_path}")
    con.execute(f"COPY final TO '{out_path.as_posix()}' (FORMAT PARQUET);")
    LOG.info("Done.")

if __name__ == "__main__":
    try:
        main()
    except duckdb.Error as e:
        fail(f"DuckDB error: {e}")
    except Exception as e:
        fail(f"Unexpected error: {e}")

